{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 27137,
                    "status": "ok",
                    "timestamp": 1704154451892,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "4FmiJ0pSimk2",
                "outputId": "95c06d80-e4b7-4fe0-dafe-b98df18b7971"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Mounted at /content/drive\n"
                    ]
                }
            ],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 7,
                    "status": "ok",
                    "timestamp": 1704154451892,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "8Lgy0bfSiqLu",
                "outputId": "302bc820-fa03-4ce1-81b8-7b7548dd9216"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/content/drive/.shortcut-targets-by-id/1zPjf1cHfdKqObemkPReffGbQHU_wotr2/NLP_Project\n"
                    ]
                }
            ],
            "source": [
                "%cd ./drive/MyDrive/Colab\\ Notebooks/NLP_Project/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "executionInfo": {
                    "elapsed": 5288,
                    "status": "ok",
                    "timestamp": 1704154457178,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "WR6a6DkN0d-3"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import torch\n",
                "from torch import nn\n",
                "import random as rnd\n",
                "from torch.optim.lr_scheduler import StepLR\n",
                "import gc"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "alY6U9G-dBSM"
            },
            "source": [
                "# Constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "executionInfo": {
                    "elapsed": 13,
                    "status": "ok",
                    "timestamp": 1704154457179,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "CL1Yi6fYdAyA"
            },
            "outputs": [],
            "source": [
                "MODEL = \"BI_LSTM\"\n",
                "NUM_LAYERS = 2\n",
                "EMBEDDING_SIZE = 300"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "executionInfo": {
                    "elapsed": 13,
                    "status": "ok",
                    "timestamp": 1704154457179,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "jS2AXWkhigQt"
            },
            "outputs": [],
            "source": [
                "def get_vocab(vocab_path, tags_path):\n",
                "    vocab = {}\n",
                "    with open(vocab_path) as f:\n",
                "        for i, l in enumerate(f.read().splitlines()):\n",
                "            vocab[l] = i  # to avoid the 0\n",
                "        # loading tags (we require this to map tags to their indices)\n",
                "    vocab['<PAD>'] = len(vocab) # 35180\n",
                "    tag_map = {}\n",
                "    with open(tags_path) as f:\n",
                "        for i, t in enumerate(f.read().splitlines()):\n",
                "            tag_map[t] = i\n",
                "\n",
                "    return vocab, tag_map\n",
                "\n",
                "def get_params(vocab, tag_map, sentences_file, labels_file):\n",
                "    sentences = []\n",
                "    labels = []\n",
                "\n",
                "    with open(sentences_file) as f:\n",
                "        for sentence in f.read().splitlines():\n",
                "            # replace each token by its index if it is in vocab\n",
                "            # else use index of UNK_WORD\n",
                "            s = [vocab[token] if token in vocab\n",
                "                 else vocab['UNK']\n",
                "                 for token in sentence.split(' ')]\n",
                "            sentences.append(s)\n",
                "\n",
                "    with open(labels_file) as f:\n",
                "        for sentence in f.read().splitlines():\n",
                "            # replace each label by its index\n",
                "            s = sentence.split(' ')\n",
                "            # remove empty strings\n",
                "            s = list(filter(None, s))\n",
                "            l = [tag_map[label] for label in s] # I added plus 1 here\n",
                "            labels.append(l)\n",
                "    return sentences, labels, len(sentences)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_44BK5K82YwF"
            },
            "source": [
                "# Importing and discovering the data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "executionInfo": {
                    "elapsed": 10666,
                    "status": "ok",
                    "timestamp": 1704154467833,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "ulSik2Sv1p1G"
            },
            "outputs": [],
            "source": [
                "\n",
                "vocab, tag_map = get_vocab('./Dataset/new_new_characters/unique_chars.txt', './Dataset/new_new_characters/unique_labels.txt')\n",
                "t_sentences, t_labels, t_size = get_params(vocab, tag_map, './Dataset/new_new_characters/t_chars.txt', './Dataset/new_new_characters/t_labels.txt')\n",
                "v_sentences, v_labels, v_size = get_params(vocab, tag_map, './Dataset/new_new_characters/v_chars.txt', './Dataset/new_new_characters/v_labels.txt')\n",
                "test_sentences1, test_labels1, test_size1 = get_params(vocab, tag_map, './Dataset/new_new_characters/test_chars.txt', './Dataset/new_new_characters/test_labels.txt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "executionInfo": {
                    "elapsed": 3482,
                    "status": "ok",
                    "timestamp": 1704154471303,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "Ae4mAnXHdrIJ"
            },
            "outputs": [],
            "source": [
                "test_sentences2, test_labels2, test_size2 = get_params(vocab, tag_map, './Dataset/new_new_characters/test_no_diacritics_chars.txt', './Dataset/new_new_characters/test_no_diacritics_labels.txt')\n",
                "test_sentences3, test_labels3, test_size3 = get_params(vocab, tag_map, './Dataset/new_new_characters/test2_chars.txt', './Dataset/new_new_characters/test2_labels.txt')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "executionInfo": {
                    "elapsed": 16,
                    "status": "ok",
                    "timestamp": 1704154471304,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "b9NQB6k9h_Ig"
            },
            "outputs": [],
            "source": [
                "# # NOTE: to increase the size of the dataset\n",
                "# t_sentences  = t_sentences + v_sentences\n",
                "# t_labels = t_labels + v_labels\n",
                "# t_size = t_size + v_size"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "wt3e4nxjFT3O"
            },
            "source": [
                "# NERDataset\n",
                "The class that impelements the dataset for NER"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "executionInfo": {
                    "elapsed": 16,
                    "status": "ok",
                    "timestamp": 1704154471304,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "29iM0u4-4YOV"
            },
            "outputs": [],
            "source": [
                "class NERDataset(torch.utils.data.Dataset):\n",
                "\n",
                "  def __init__(self, x, y, pad):\n",
                "    self.x = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in x], padding_value=pad,batch_first = True)\n",
                "    self.y = nn.utils.rnn.pad_sequence([torch.tensor(i) for i in y], padding_value=tag_map[\"pad\"],batch_first = True)\n",
                "    print('The max length of the sentences is', self.x.shape[1])\n",
                "    print('The max length of the labels is', self.y.shape[1])\n",
                "  def __len__(self):\n",
                "    return len(self.x)\n",
                "\n",
                "  def __getitem__(self, idx):\n",
                "    return self.x[idx], self.y[idx]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CQB6O7I7FbUh"
            },
            "source": [
                "# Classifiers\n",
                "The class that implementss the pytorch model for arabic diacritic classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "executionInfo": {
                    "elapsed": 15,
                    "status": "ok",
                    "timestamp": 1704154471304,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "xHeJcz1JuhYa"
            },
            "outputs": [],
            "source": [
                "class ArabicDiacriticsClassifier(nn.Module):\n",
                "  def __init__(self, vocab_size=len(t_sentences) + len(v_sentences) + len(v_sentences), num_layers = 3, embedding_dim = 512, hidden_size=256, n_classes=len(tag_map)):\n",
                "    super(ArabicDiacriticsClassifier, self).__init__()\n",
                "    # (1) Create the embedding layer\n",
                "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
                "    # import gensim.downloader as api\n",
                "    # self.embedding.weight.data.copy_(torch.from_numpy(api.load('word2vec-google-news-300').vectors[:1000])) WORD2VEc\n",
                "    # import fasttext.util\n",
                "    # self.embedding.weight.data.copy_(torch.from_numpy(fasttext.util.download_model('en', if_exists='ignore').get_input_matrix()[:1000]))\n",
                "\n",
                "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
                "    self.num_layers = num_layers\n",
                "    self.hidden_size = hidden_size\n",
                "\n",
                "\n",
                "    self.lstm = nn.LSTM(embedding_dim, hidden_size,num_layers, batch_first=True, bidirectional=True)\n",
                "    self.linear = nn.Linear(2 * hidden_size, n_classes)\n",
                "\n",
                "    # (3) Create a linear layer with number of neorons = n_classes\n",
                "    # self.linear = nn.Linear(hidden_size, n_classes)\n",
                "\n",
                "\n",
                "  def forward(self, sentences):\n",
                "    embeddings = self.embedding(sentences)\n",
                "\n",
                "    # BIDIRECTIONAL\n",
                "    # Initialize hidden states for bidirectional LSTM\n",
                "    # h0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n",
                "    # c0 = torch.zeros(self.num_layers*2, embeddings.size(0), self.hidden_size).to(sentences.device)\n",
                "    # lstm_out, (a, b) = self.lstm(embeddings, (h0, c0))\n",
                "\n",
                "    # LSTM\n",
                "    lstm1_out, (h_n, c_n) = self.lstm(embeddings)\n",
                "    final_output = self.linear(lstm1_out)\n",
                "    # final_output = self.linear(lstm_out[:, -1, :])\n",
                "    return final_output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 15,
                    "status": "ok",
                    "timestamp": 1704154471304,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "lJJJF-qQA_wk",
                "outputId": "d7cca685-9cb8-4628-e55f-fad854474d8a"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ArabicDiacriticsClassifier(\n",
                        "  (embedding): Embedding(128137, 512)\n",
                        "  (lstm): LSTM(512, 256, num_layers=3, batch_first=True, bidirectional=True)\n",
                        "  (linear): Linear(in_features=512, out_features=16, bias=True)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "model = ArabicDiacriticsClassifier()\n",
                "print(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 3,
                    "status": "ok",
                    "timestamp": 1704154631863,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "1OFu3LEnjqor",
                "outputId": "b64116eb-fe93-4a63-c67e-80adde1738a4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "5770\n"
                    ]
                }
            ],
            "source": [
                "test_sentences = test_sentences1\n",
                "test_labels = test_labels1\n",
                "print(len(test_sentences3))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 635,
                    "status": "ok",
                    "timestamp": 1704154632497,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "3BI7_ANkLf7G",
                "outputId": "bdc81f75-771f-47a3-e918-42e97a40cce2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The max length of the sentences is 1936\n",
                        "The max length of the labels is 1936\n"
                    ]
                }
            ],
            "source": [
                "test_dataset = NERDataset(test_sentences, test_labels, vocab['<PAD>'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "executionInfo": {
                    "elapsed": 2,
                    "status": "ok",
                    "timestamp": 1704154632497,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "b2Lj2SneWZUj"
            },
            "outputs": [],
            "source": [
                "def load_model(model,model_name):\n",
                "  model.load_state_dict(torch.load(f'./SavedModels/{model_name}'))\n",
                "  return model\n",
                "def load_baseline_epoch_model(model,model_name):\n",
                "  model.load_state_dict(torch.load(f'./BaseLineModels/{model_name}'))\n",
                "  return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {
                "executionInfo": {
                    "elapsed": 6097,
                    "status": "ok",
                    "timestamp": 1704154707378,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "NtFqGvvgZVv-"
            },
            "outputs": [],
            "source": [
                "model_name = \"model_baseline_EPOCH6\"\n",
                "# model_name = \"model_3_baseLine_batch256_lr0.001_embedding_512_epoch1\"\n",
                "model = load_baseline_epoch_model(model, model_name)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TWJNO6mUXPRI"
            },
            "source": [
                "# Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "executionInfo": {
                    "elapsed": 14,
                    "status": "ok",
                    "timestamp": 1704154707378,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "Gz5mxUAJM1xS"
            },
            "outputs": [],
            "source": [
                "diacritic_results = []\n",
                "gold_results = []\n",
                "test_input_list = []\n",
                "\n",
                "def evaluate(model, test_dataset, batch_size=64):\n",
                "  # (1) create the test data loader\n",
                "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
                "\n",
                "  # GPU Configuration\n",
                "  use_cuda = torch.cuda.is_available()\n",
                "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
                "  if use_cuda:\n",
                "    model = model.cuda()\n",
                "\n",
                "  total_acc_test = 0\n",
                "\n",
                "\n",
                "  # (2) disable gradients\n",
                "  with torch.no_grad():\n",
                "\n",
                "    for test_input, test_label in tqdm(test_dataloader):\n",
                "      # (3) move the test input to the device\n",
                "      test_label = test_label.to(device)\n",
                "\n",
                "      # (4) move the test label to the device\n",
                "      test_input = test_input.to(device)\n",
                "\n",
                "      # (5) do the forward pass\n",
                "      output = model(test_input)\n",
                "      prediction = output.argmax(2)\n",
                "\n",
                "\n",
                "      diacritic_results.extend(np.array(prediction.cpu().data).flatten())\n",
                "      gold_results.extend(np.array(test_label.cpu().data).flatten())\n",
                "      test_input_list.extend(np.array(test_input.cpu().data).flatten())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DMsZQ4ITWAEl"
            },
            "source": [
                "### Evaluatio and Save"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {
                "executionInfo": {
                    "elapsed": 13,
                    "status": "ok",
                    "timestamp": 1704154707378,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "bHY019YvKejN"
            },
            "outputs": [],
            "source": [
                "def DER():\n",
                "  der = 0\n",
                "  total_size = 0\n",
                "  for i in range(len(diacritic_results)):\n",
                "    if test_input_list[i] != vocab['<PAD>']: # Do not include padding in DER calculations\n",
                "      if diacritic_results[i] != gold_results[i] : # Miss Classification\n",
                "        der += 1\n",
                "      total_size += 1\n",
                "  der /= total_size\n",
                "  der *= 100\n",
                "  print(\"DER = \",der,\"%\")\n",
                "  print(\"Accuracy = \",100 - der,\"%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "executionInfo": {
                    "elapsed": 13,
                    "status": "ok",
                    "timestamp": 1704154707378,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "LLtlWNm6YVG2"
            },
            "outputs": [],
            "source": [
                "filtered_diacritic_results = [] # diacrtic results without paddings\n",
                "filtered_inputs = [] # inputs without paddings\n",
                "def PerpareForExportToCSV():\n",
                "  # Prepare the data that will be written in the CSV file\n",
                "  # these list are sorted as mentioned by the TA\n",
                "  LIST_OF_DIACRITICS = [\n",
                "      \"FATHA\",\n",
                "      \"FATHATAN\",\n",
                "      \"DAMMA\",\n",
                "      \"DAMMATAN\",\n",
                "      \"KASRA\",\n",
                "      \"KASRATAN\",\n",
                "      \"SUKUN\",\n",
                "      \"SHADDA\",\n",
                "      \"SHADDA_FATHA\",\n",
                "      \"SHADDA_FATHATAN\",\n",
                "      \"SHADDA_DAMMA\",\n",
                "      \"SHADDA_DAMMATAN\",\n",
                "      \"SHADDA_KASRA\",\n",
                "      \"SHADDA_KASRATAN\",\n",
                "      \"_\"\n",
                "  ]\n",
                "  LIST_OF_ARABIC_LETTERS = list(vocab.keys())\n",
                "\n",
                "\n",
                "\n",
                "  for i in range(len(diacritic_results)):\n",
                "    if test_input_list[i] != vocab['<PAD>']:\n",
                "      filtered_diacritic_results.append(diacritic_results[i])\n",
                "      filtered_inputs.append(test_input_list[i])\n",
                "\n",
                "  index = len(filtered_diacritic_results)\n",
                "\n",
                "  inputs = [LIST_OF_ARABIC_LETTERS[filtered_inputs[i]] for i in range(index)]\n",
                "  model_prediction = [LIST_OF_DIACRITICS[filtered_diacritic_results[i]] for i in range(index)]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "executionInfo": {
                    "elapsed": 12,
                    "status": "ok",
                    "timestamp": 1704154707378,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "vzd36snRjME9"
            },
            "outputs": [],
            "source": [
                "def ExportToCSV(model_name):\n",
                "  data_length = len(filtered_diacritic_results)\n",
                "  assert data_length == 417359, f\"Expected data length to be 417359, but got {data_length}.\"\n",
                "  df = pd.DataFrame(\n",
                "      {\n",
                "      'ID': range(len(filtered_diacritic_results[0: data_length])),\n",
                "      'label': filtered_diacritic_results[0: data_length],\n",
                "      })\n",
                "\n",
                "  df.to_csv(f'./Results/result_{model_name}.csv', index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 48717,
                    "status": "ok",
                    "timestamp": 1704154756083,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "88xgvIOwjJ9J",
                "outputId": "14981012-7ca5-4c6c-97c7-565b1a2f22c8"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 96/96 [00:48<00:00,  1.97it/s]\n"
                    ]
                }
            ],
            "source": [
                "evaluate(model, test_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "executionInfo": {
                    "elapsed": 4000,
                    "status": "ok",
                    "timestamp": 1704154760069,
                    "user": {
                        "displayName": "Mark Test",
                        "userId": "07605231817608634761"
                    },
                    "user_tz": -120
                },
                "id": "GbOfNxV4b-eC",
                "outputId": "50963300-889c-4fc7-8ddb-33076414fae2"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calculating DER for our test set\n",
                        "DER =  3.018976760327245 %\n",
                        "Accuracy =  96.98102323967275 %\n"
                    ]
                }
            ],
            "source": [
                "if len(test_sentences2) != len(test_sentences): # If we are testing on our test sets\n",
                "  print(\"Calculating DER for our test set\")\n",
                "  DER()\n",
                "else:\n",
                "  print(\"Exporting to CSV ...\")\n",
                "  model_name = f\"model_baseline\"\n",
                "  PerpareForExportToCSV()\n",
                "  ExportToCSV(model_name)\n",
                "  print(\"Exported Successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "JXIAp6PCiBjs"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}